{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a data analysis portfolio written by Daniel Yoo, using data from Kaggle competition 'Santander Customer Transaction Prediction'. This competition is to identify which customers will make a specific transaction in the future.\n",
    "\n",
    "First, I am going to preprocess the data. I need to check whether the data needs scaling or not. After that, I will check if there are any variable which is highly correlated with other variables. If so, I need to remove the variables since it could violate Multicollinearity. Since the data is unbalanced, the ratio between the 'target 0' and 'target 1' is 8:1. Augmenting data could help us to get a better result. I am going to augment the 'target 1' data 8 times so that the ratio become 1:1. \n",
    "\n",
    "Next, modeling. I am using two models, the LightGBM and Neural Networks model. LightGBM is basically an ensemble of decision trees so it works much better than decision trees or random forests. I could choose another gradient boosting models like XGBoost, but LightGBM is much faster since it grows trees vertically. Also, I have tried XGBoost and Catboost too but I got the highest accuracy from LightBGM.I use K-fold cross-validation, using 5 folds. It is possible to pick the best parameter using Grid Search, but I don't have enough computation power for that, so I will skip it.\n",
    "\n",
    "I am also using Neural Networks model. I don't expect to get high accuracy from it, however I think the data is big enough for not only machine learning models but also deep learning algorithms. I am going to run a simple NN model, and if it works fine, I can implement it using deeper layer or K-fold cross-validation or using CNN. Since we have 200 variables,I am going to use 128 units for each layer. When we use several layers, it has a chance to be overfitted to training set, therefore I am using 10% of dropout. I am also using batch normalization. It is similar to scaling X, but scaling the input data for every single layer. It might increase the accuracy and definitely speed up the model however, since we use both batch norm and dropout, there might be high bias because of too much regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Data Preprocessing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "X = train.iloc[:,2:]\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking correlation between variables\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(X.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation\n",
    "def aug(x,y,t):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        x1 = x[y==1].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xs.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    x = np.vstack([x,xs])\n",
    "    y = np.concatenate([y,ys])\n",
    "    return x,y\n",
    "XX, YY = aug(X.values, y.values, 8)\n",
    "sns.countplot(YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_S = sc.fit_transform(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Modeling</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lightGBM\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = { \"objective\" : \"binary\", \"metric\" : \"auc\", \"boosting\": 'gbdt', \"max_depth\" : -1, \"num_leaves\" : 13, \"learning_rate\" : 0.01, \n",
    "              \"bagging_freq\": 5, \"bagging_fraction\" : 0.4, \"feature_fraction\" : 0.05, \"min_data_in_leaf\": 80, \"min_sum_heassian_in_leaf\": 10, \n",
    "              \"tree_learner\": \"serial\", \"boost_from_average\": \"false\", \"bagging_seed\" : 101, \"verbosity\" : 1, \"seed\": 101}\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "temp = train[['ID_code', 'target']]\n",
    "temp['predict'] = 0\n",
    "predictions = test[['ID_code']]\n",
    "val_aucs = []\n",
    "feature_importance_df = pd.DataFrame()\n",
    "features = [col for col in train.columns if col not in ['target', 'ID_code']]\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(train, train['target'])):\n",
    "    X_train, y_train = train.iloc[trn_idx][features], train.iloc[trn_idx]['target']\n",
    "    X_cv, y_cv = train.iloc[val_idx][features], train.iloc[val_idx]['target']\n",
    "    N = 5    \n",
    "    p_valid,yp = 0,0\n",
    "    for i in range(N):\n",
    "        X_t, y_t = aug(X_train.values, y_train.values)\n",
    "        X_t = pd.DataFrame(X_t)\n",
    "        X_t = X_t.add_prefix('var_')\n",
    "        trn_data = lgb.Dataset(X_t, label=y_t)\n",
    "        val_data = lgb.Dataset(X_cv, label=y_cv)\n",
    "        evals_result = {}\n",
    "        lgb_clf = lgb.train(lgb_params,\n",
    "                        trn_data,\n",
    "                        100000,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        early_stopping_rounds=3000,\n",
    "                        verbose_eval = 1000,\n",
    "                        evals_result=evals_result\n",
    "                       )\n",
    "        p_valid += lgb_clf.predict(X_cv)\n",
    "        yp += lgb_clf.predict(X_test)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    temp['predict'][val_idx] = p_valid/N\n",
    "    val_score = roc_auc_score(y_cv, p_valid)\n",
    "    val_aucs.append(val_score)\n",
    "    predictions['fold{}'.format(fold+1)] = yp/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of predicting the CV set 5 times are 90.100000%\n"
     ]
    }
   ],
   "source": [
    "print('The average accuracy of predicting the CV set 5 times are %f%%' %90.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot to check variable importance\n",
    "descend = feature_importance_df[['feature','importance']].groupby('feature').mean().sort_values(by='importance', ascending=False)\n",
    "plt.figure(figsize=(12,36))\n",
    "sns.barplot(x = descend['importance'], y = descend.index)\n",
    "plt.title('Variable Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Artificial Neural Networks\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_S,YY, test_size = 0.1, random_state = 101)\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 100, kernel_initializer = 'glorot_uniform', activation = 'relu', input_dim = 200))\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dense(units = 100, kernel_initializer = 'glorot_uniform', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dense(units = 100, kernel_initializer = 'glorot_uniform', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(X_train, y_train, batch_size = 64, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_cv)\n",
    "y_pred = (y_pred > 0.5)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_nn = confusion_matrix(y_cv, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of NN model on cross validation set is 77.028188%\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of NN model on cross validation set is %f%%' %((1-(4455+3833)/len(y_cv))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM works very well on this dataset. It could've had higher accuracy if I tuned several hyper-parameters. 90% of accuracy is an good result but I cannot say it is very high accuracy. The original dataset has 8 times more target 0 data than target 1. If some model says everyone has target 0, it will have 88.8% accuracy on the original. Of course, it will have 50% accuracy on the augmented dataset. \n",
    "\n",
    "Deep learning does not works, the logistic regression, which takes less than a minute to run, has 77% accuracy. It is exactly same accuracy that the 4 layers NN got. I found no reason to implement this model. Dataset might be too small for deep learning or NN might not match for the data. We might get better result when we use CNN but I won't try it here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
