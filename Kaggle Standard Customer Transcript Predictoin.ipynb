{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is a data analysis portfolio written by Daniel Yoo, using data from Kaggle competition 'Santander Customer Transaction Prediction.' This competition is to identify which customers will make a specific transaction in the future.\n",
    "\n",
    "First, I preprocessed the data. The dataset doesn't contain outliers or NULL. However, it is very unbalanced. The dataset doesn't have enough 'target 1' data. Data of 'target 0' appears more frequently than data of 'target 1'; the ratio between the number of each data point is 8:1. It needs to be scaled. If we build a model without it, the model cannot be trained target 1 appropriately and lead to poor accuracy. Augmenting data could help to solve this problem. I randomly picked 'target 1' data from the dataset and replicated it until the ratio becomes one to one. \n"
    "\n",
    "I am using two models in this report, the LightGBM and the Neural Networks model. I have also tried another gradient boosting model like XGBoost, but in this dataset, LightGBM performs better. I used K-fold cross-validation, with five folds. I choose not to do the Grid Search because of the lacking computing power, but it would've helped the model if I had done it. \n"
    "\n",
    "I am also using the Neural Networks model to compare the result with the last one. I do not have a high expectation of this model, but it is worth to try a deep learning algorithm since the data set is big enough. Comparing to the last model, this one will be much simpler. It is a 4-layer NN model with 128 units each. On each layer, the dropout rate is 10 percent, and input data is regulated with batch normalization. I've tried deeper layers too, but it doesn't help the model, so I decide to stick with four layers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Data Preprocessing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "X = train.iloc[:,2:]\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking correlation between variables\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(X.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation\n",
    "def aug(x,y,t):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        x1 = x[y==1].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xs.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    x = np.vstack([x,xs])\n",
    "    y = np.concatenate([y,ys])\n",
    "    return x,y\n",
    "XX, YY = aug(X.values, y.values, 8)\n",
    "sns.countplot(YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_S = sc.fit_transform(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Modeling</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lightGBM\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = { \"objective\" : \"binary\", \"metric\" : \"auc\", \"boosting\": 'gbdt', \"max_depth\" : -1, \"num_leaves\" : 13, \"learning_rate\" : 0.01, \n",
    "              \"bagging_freq\": 5, \"bagging_fraction\" : 0.4, \"feature_fraction\" : 0.05, \"min_data_in_leaf\": 80, \"min_sum_heassian_in_leaf\": 10, \n",
    "              \"tree_learner\": \"serial\", \"boost_from_average\": \"false\", \"bagging_seed\" : 101, \"verbosity\" : 1, \"seed\": 101}\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "temp = train[['ID_code', 'target']]\n",
    "temp['predict'] = 0\n",
    "predictions = test[['ID_code']]\n",
    "val_aucs = []\n",
    "feature_importance_df = pd.DataFrame()\n",
    "features = [col for col in train.columns if col not in ['target', 'ID_code']]\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(train, train['target'])):\n",
    "    X_train, y_train = train.iloc[trn_idx][features], train.iloc[trn_idx]['target']\n",
    "    X_cv, y_cv = train.iloc[val_idx][features], train.iloc[val_idx]['target']\n",
    "    N = 5    \n",
    "    p_valid,yp = 0,0\n",
    "    for i in range(N):\n",
    "        X_t, y_t = aug(X_train.values, y_train.values)\n",
    "        X_t = pd.DataFrame(X_t)\n",
    "        X_t = X_t.add_prefix('var_')\n",
    "        trn_data = lgb.Dataset(X_t, label=y_t)\n",
    "        val_data = lgb.Dataset(X_cv, label=y_cv)\n",
    "        evals_result = {}\n",
    "        lgb_clf = lgb.train(lgb_params,\n",
    "                        trn_data,\n",
    "                        100000,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        early_stopping_rounds=3000,\n",
    "                        verbose_eval = 1000,\n",
    "                        evals_result=evals_result\n",
    "                       )\n",
    "        p_valid += lgb_clf.predict(X_cv)\n",
    "        yp += lgb_clf.predict(X_test)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    temp['predict'][val_idx] = p_valid/N\n",
    "    val_score = roc_auc_score(y_cv, p_valid)\n",
    "    val_aucs.append(val_score)\n",
    "    predictions['fold{}'.format(fold+1)] = yp/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of predicting the CV set 5 times are 90.100000%\n"
     ]
    }
   ],
   "source": [
    "print('The average accuracy of predicting the CV set 5 times are %f%%' %90.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot to check variable importance\n",
    "descend = feature_importance_df[['feature','importance']].groupby('feature').mean().sort_values(by='importance', ascending=False)\n",
    "plt.figure(figsize=(12,36))\n",
    "sns.barplot(x = descend['importance'], y = descend.index)\n",
    "plt.title('Variable Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Artificial Neural Networks\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_S,YY, test_size = 0.1, random_state = 101)\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 100, kernel_initializer = 'glorot_uniform', activation = 'relu', input_dim = 200))\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dense(units = 100, kernel_initializer = 'glorot_uniform', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dense(units = 100, kernel_initializer = 'glorot_uniform', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(X_train, y_train, batch_size = 64, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_cv)\n",
    "y_pred = (y_pred > 0.5)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_nn = confusion_matrix(y_cv, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of NN model on cross validation set is 77.028188%\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of NN model on cross validation set is %f%%' %((1-(4455+3833)/len(y_cv))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM works very well on this dataset. It could've had higher accuracy if I tuned several hyper-parameters. 90% of accuracy is an good result but I cannot say it is very high accuracy. The original dataset has 8 times more target 0 data than target 1. If some model says everyone has target 0, it will have 88.8% accuracy on the original. Of course, it will have 50% accuracy on the augmented dataset. \n",
    "\n",
    "Deep learning does not works, the logistic regression, which takes less than a minute to run, has 77% accuracy. It is exactly same accuracy that the 4 layers NN got. I found no reason to implement this model. Dataset might be too small for deep learning or NN might not match for the data. We might get better result when we use CNN but I won't try it here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
